{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CREATE KEYSPACE streaming_vehicle_report WITH replication = {'class': 'SimpleStrategy', 'replication_factor':1};\n",
    "USE streaming_vehicle_report;\n",
    "CREATE TABLE cosit_class_count(time text, cosit text, classname text, count int, primary key(time, cosit, count));\n",
    "CREATE TABLE cosit_class_averagesp(time text, cosit text, classname text, averagesp float, primary key(time, cosit, averagesp));\n",
    "CREATE TABLE busy_sites(time text, cosit text, count int, primary key(time, cosit, count));\n",
    "CREATE TABLE count_hgb(time text, count int, primary key(time, count));\n",
    "\"\"\"\n",
    "from pyspark.sql import *\n",
    "\n",
    "def cosit_class_count(time, RDD):\n",
    "    if not RDD.isEmpty():\n",
    "        rdd = RDD.map(lambda row: Row(time=time, cosit=row[0][0], classname=row[0][1], count=row[1]))\n",
    "        dataframe = spark.createDataFrame(rdd)\n",
    "        dataframe.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"cosit_class_count\", keyspace=\"streaming_vehicle_report\").save(mode=\"append\")\n",
    "    \n",
    "def cosit_class_averagesp(time, RDD):\n",
    "    if not RDD.isEmpty():\n",
    "        rdd = RDD.map(lambda row: Row(time=time, cosit=row[0][0], classname=row[0][1], averagesp=row[1]))\n",
    "        dataframe = spark.createDataFrame(rdd)\n",
    "        dataframe.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"cosit_class_averagesp\", keyspace=\"streaming_vehicle_report\").save(mode=\"append\")\n",
    "    \n",
    "def busy_sites(time, RDD):\n",
    "    if not RDD.isEmpty():\n",
    "        rdd = RDD.map(lambda row: Row(time=time, cosit=row[0], count=row[1]))\n",
    "        dataframe = spark.createDataFrame(rdd)\n",
    "        dataframe.limit(3).write.format(\"org.apache.spark.sql.cassandra\").options(table=\"busy_sites\", keyspace=\"streaming_vehicle_report\").save(mode=\"append\")\n",
    "    \n",
    "def count_hgb(time, RDD):\n",
    "    if not RDD.isEmpty():\n",
    "        rdd = RDD.map(lambda row: Row(time=time, count=row[1]))\n",
    "        dataframe = spark.createDataFrame(rdd)\n",
    "        dataframe.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"count_hgb\", keyspace=\"streaming_vehicle_report\").save(mode=\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "header = 'cosit,year,month,day,hour,minute,second,millisecond,minuteofday,lane,lanename,straddlelane,straddlelanename,class,classname,length,headway,gap,speed,weight,temperature,duration,validitycode,numberofaxles,axleweights,axlespacings'\n",
    "\n",
    "# to create dictionary of (line from streaming and its header)\n",
    "def to_dict(header, line):\n",
    "    zipped = zip(header.split(','), map(lambda value: value.strip(\"\\\"\"), line.split(\",\")))\n",
    "    return dict(zipped)\n",
    "\n",
    "\n",
    "# creating spark streaming context from spark context with 5 secs.\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# for development\n",
    "# lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# production\n",
    "lines = ssc.textFileStream(\"./streams/\")\n",
    "\n",
    "# creating dictionary so its easy to access data later.\n",
    "dict_data = lines.map(lambda line: to_dict(header, line))\n",
    "\n",
    "\n",
    "# filtering records for M50\n",
    "m50_data = dict_data.filter(lambda row:row['cosit'].lstrip('0') in ['1012','1500','1501','1502','1503','1504','1505','1506','1507','1508','1509','15010','15011','15012'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show total number of counts (on each site of M50) by vehicle class.\n",
    "\n",
    "# grouping m50 data by cosit and classname, and merging rows into lists\n",
    "# this creates data in format (('xxxcosit','xxxclassname'),[{xxxrow}, {xxxrow}, ..., {xxxrow}])\n",
    "m50_grouped_by_classname_cosit = m50_data.map(lambda row: ((row['cosit'],row['classname']), row)).map(lambda row : (row[0], [row[1]])).reduceByKey(lambda a,b:a+b)\n",
    "# better thing to do is \n",
    "# ** m50_grouped_by_classname_cosit = m50_data.map(lambda row: ((row['cosit'],row['classname']), 1)).reduceByKey(lambda a,b:a+b)\n",
    "\n",
    "# taking key and length of value from above DStream\n",
    "m50_vehicle_count = m50_grouped_by_classname_cosit.map(lambda row: (row[0], len(row[1])))\n",
    "# ** if we do as marked as (**) above then\n",
    "# ** m50_grouped_by_classname_cosit = m50_data.map(lambda row: ((row['cosit'],row['classname']), 1)).reduceByKey(lambda a,b:a+b)\n",
    "\n",
    "# displaying in console\n",
    "m50_vehicle_count.pprint()\n",
    "\n",
    "# saving each record into cassandra\n",
    "m50_vehicle_count.foreachRDD(cosit_class_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show average speed (on each site of M50) by vehicle class.\n",
    "\n",
    "# again grouping m50 data by cosit and classname, and merging only speed into lists\n",
    "m50_grouped_by_classname_cosit_speed = m50_data.map(lambda row: ((row['cosit'],row['classname']), float(row['speed']))).map(lambda row : (row[0], [row[1]])).reduceByKey(lambda a,b:a+b)\n",
    "# average_speed = sum/length\n",
    "m50_grouped_by_classname_cosit_speed_averaged = m50_grouped_by_classname_cosit_speed.map(lambda row: (row[0], sum(row[1])/len(row[1])))\n",
    "# displaying in console\n",
    "m50_grouped_by_classname_cosit_speed_averaged.pprint()\n",
    "# saving each record into cassandra\n",
    "m50_grouped_by_classname_cosit_speed_averaged.foreachRDD(cosit_class_averagesp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 3 busiest counter sites on M50\n",
    "\n",
    "# grouping by cosit and counting records belonging to cosit\n",
    "m50_grouped_by_cosit_count = m50_data.map(lambda row: (row['cosit'], 1)).reduceByKey(lambda a,b:a+b)\n",
    "# sorting data in descending order by key.\n",
    "m50_grouped_by_cosit_count_sorted = m50_grouped_by_cosit_count.transform(lambda rdd: rdd.sortByKey(ascending=False))\n",
    "# displaying in console\n",
    "m50_grouped_by_cosit_count_sorted.pprint()\n",
    "# saving each record into cassandra\n",
    "m50_grouped_by_cosit_count_sorted.foreachRDD(busy_sites)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Find total number of counts of HGV on M50\n",
    "\n",
    "# filtering out HGV's from m50_data, and counting records\n",
    "m50_total_HGV = m50_data.filter(lambda row: row['classname'].strip().upper() == 'HGV_RIG').map(lambda row:('HGV_RIG',1)).reduceByKey(lambda a,b:a+b)\n",
    "# displaying data in console.\n",
    "m50_total_HGV.pprint()\n",
    "# saving data into cassandra\n",
    "m50_total_HGV.foreachRDD(count_hgb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-05-04 08:12:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-05-04 08:12:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-05-04 08:12:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-05-04 08:12:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-05-04 08:12:45\n",
      "-------------------------------------------\n",
      "(('000000001012', 'HGV_ART'), 1)\n",
      "(('000000001507', 'BUS'), 1)\n",
      "(('000000001503', 'HGV_ART'), 3)\n",
      "(('000000001506', 'CAR'), 8)\n",
      "(('000000001506', 'HGV_RIG'), 1)\n",
      "(('000000001509', 'LGV'), 1)\n",
      "(('000000001509', 'HGV_ART'), 2)\n",
      "(('000000015012', 'LGV'), 2)\n",
      "(('000000001504', 'LGV'), 3)\n",
      "(('000000015010', 'HGV_ART'), 3)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f2feb7cf9e68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# running for 600 seconds = 10 min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# you probably may have to stop the kernel manually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# stoping sparkstreamingcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopSparkContext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# starting execution\n",
    "ssc.start()\n",
    "# running for 600 seconds = 10 min\n",
    "# you probably may have to stop the kernel manually\n",
    "time.sleep(600)\n",
    "# stoping sparkstreamingcontext\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
